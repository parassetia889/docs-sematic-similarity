{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parassetia889/docs-sematic-similarity/blob/main/similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POjpL0moQbV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04b8d33-f65f-4575-a93c-f5e15c8f7704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install PyPDF2\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw0cyo1kwLRh",
        "outputId": "53613edc-a7bd-4568-a6a5-cbe33796b9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Time : 1707904763.7720938\n",
            "content fetched\n",
            "Time Taken: 51.7560 seconds\n",
            "Similarity between documents: 64.00%\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import xlrd\n",
        "\n",
        "def extract_content(file_path, format):\n",
        "    if format == \"pdf\":\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    elif format == \"xls\":\n",
        "        workbook = xlrd.open_workbook(file_path)\n",
        "        text = \"\"\n",
        "        for sheet_name in workbook.sheet_names():\n",
        "            sheet = workbook.sheet_by_name(sheet_name)\n",
        "            for row_idx in range(sheet.nrows):\n",
        "                for col_idx in range(sheet.ncols):\n",
        "                    cell_value = sheet.cell_value(row_idx, col_idx)\n",
        "                    if cell_value:\n",
        "                        text += str(cell_value) + \" \"\n",
        "        return text\n",
        "    elif format =='txt':\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return text\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format:\", format)\n",
        "\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=500):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i:i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def encode_and_combine_chunks(chunks, model):\n",
        "    chunk_embeddings = model.encode(chunks)\n",
        "    combined_embedding = np.mean(chunk_embeddings, axis=0)  # Average embeddings\n",
        "    return combined_embedding\n",
        "\n",
        "\n",
        "def compare_documents(file1_path, file2_path, chunk_size=500):\n",
        "    start_time = time.time()\n",
        "    print(f\"Start Time : {start_time}\")\n",
        "    model = SentenceTransformer(\"paraphrase-distilroberta-base-v2\")  # Use paraphrase model for long text\n",
        "\n",
        "    # Extract text from PDFs\n",
        "    format = file1_path.split(\".\")[1]\n",
        "    doc1_content = extract_content(file1_path, format)\n",
        "    doc2_content = extract_content(file2_path, format)\n",
        "\n",
        "    print(\"content fetched\")\n",
        "    # Split text into chunks\n",
        "    chunk1 = split_text_into_chunks(doc1_content, chunk_size)\n",
        "    chunk2 = split_text_into_chunks(doc2_content, chunk_size)\n",
        "\n",
        "    # Encode chunks and combine embeddings\n",
        "    content1_embedding = encode_and_combine_chunks(chunk1, model)\n",
        "    content2_embedding = encode_and_combine_chunks(chunk2, model)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(content1_embedding.reshape(1, -1), content2_embedding.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Print similarity score\n",
        "    similarity_percentage = round(similarity * 100, 2)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\"Time Taken: {:.4f} seconds\".format(elapsed_time))\n",
        "    print(f\"Similarity between documents: {similarity_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# pdf1_path = \"/content/file1.pdf\"\n",
        "# pdf2_path = \"/content/file2.pdf\"\n",
        "\n",
        "# compare_documents(pdf1_path, pdf2_path)\n",
        "\n",
        "\n",
        "pdf1_path = \"/content/Hist.txt\"\n",
        "pdf2_path = \"/content/Moby.txt\"\n",
        "\n",
        "compare_documents(pdf1_path, pdf2_path)\n",
        "\n",
        "# file1_path = \"/content/file1.xls\"\n",
        "# file2_path = \"/content/file2.xls\"\n",
        "\n",
        "# compare_documents(file1_path, file2_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyZlgcm8207R",
        "outputId": "c2194bc5-17bd-43d9-b6e3-c722d0910899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc1_content : 24111\n",
            "doc2_content : 12302\n",
            "Time Taken: 3.8867 seconds\n",
            "pdf  Score :  98.68 \n",
            "\n",
            "doc1_content : 1283\n",
            "doc2_content : 34424\n",
            "Time Taken: 4.3826 seconds\n",
            "Novel txt Score :  47.32 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def extract_content(file_path, format):\n",
        "    if format == \"pdf\":\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                page = reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    elif format == \"txt\":\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()\n",
        "    else:\n",
        "        print(\"format : \", format)\n",
        "        raise ValueError(f\"Unsupported file format: {format}\")\n",
        "\n",
        "\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=200):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i:i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def encode_and_combine_chunks(chunks, model):\n",
        "    chunk_embeddings = model.encode(chunks)\n",
        "    combined_embedding = np.mean(chunk_embeddings, axis=0)  # Average embeddings\n",
        "    return combined_embedding\n",
        "\n",
        "\n",
        "def compare_documents(file1_path, file2_path, chunk_size=100):\n",
        "    start_time = time.time()\n",
        "    # model = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v2\")  # Use paraphrase model for long text\n",
        "    # model = SentenceTransformer(\"castorini/aggretriever-distilbert\")\n",
        "    model = SentenceTransformer(\"paraphrase-distilroberta-base-v2\")  # Use paraphrase model for long text\n",
        "    model = SentenceTransformer(\"all-mpnet-base-v2\")  # Use paraphrase model for long text\n",
        "    # model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "    # Extract text from PDFs\n",
        "    format = file1_path.split(\".\")[1]\n",
        "    doc1_content = extract_content(file1_path, format)\n",
        "    doc2_content = extract_content(file2_path, format)\n",
        "\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunk1 = split_text_into_chunks(doc1_content, chunk_size)\n",
        "    chunk2 = split_text_into_chunks(doc2_content, chunk_size)\n",
        "\n",
        "    print(f\"doc1_content : {len(doc1_content)}\")\n",
        "    print(f\"doc2_content : {len(doc2_content)}\")\n",
        "    # Encode chunks and combine embeddings\n",
        "    content1_embedding = encode_and_combine_chunks(chunk1, model)\n",
        "    content2_embedding = encode_and_combine_chunks(chunk2, model)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(content1_embedding.reshape(1, -1), content2_embedding.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Print similarity score\n",
        "    similarity_percentage = round(similarity * 100, 2)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\"Time Taken: {:.4f} seconds\".format(elapsed_time))\n",
        "    return similarity_percentage\n",
        "\n",
        "\n",
        "# file1_path = \"/content/3page.txt\"\n",
        "# file2_path = \"/content/1page.txt\"\n",
        "# print(\"pdf to txt  Score : \", compare_documents(file1_path, file2_path), \"\\n\")\n",
        "\n",
        "file1_path = \"/content/file1.pdf\"\n",
        "file2_path = \"/content/file1_removed.pdf\"\n",
        "print(\"pdf  Score : \", compare_documents(file1_path, file2_path), \"\\n\")\n",
        "# # Example usage\n",
        "# file1_path = \"/content/hello.txt\"\n",
        "# file2_path = \"/content/hello2.txt\"\n",
        "# print(\"hello txt Score : \", compare_documents(file1_path, file2_path), \"\\n\")\n",
        "\n",
        "# file1_path = \"/content/unique_file_ids 1.txt\"\n",
        "# file2_path = \"/content/unique_file_ids 2.txt\"\n",
        "# print(\"long txt Score : \", compare_documents(file1_path, file2_path), \"\\n\")\n",
        "\n",
        "file1_path = \"/content/Hist.txt\"\n",
        "file2_path = \"/content/Moby.txt\"\n",
        "print(\"Novel txt Score : \", compare_documents(file1_path, file2_path), \"\\n\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}